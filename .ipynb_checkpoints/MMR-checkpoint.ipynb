{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a12444-be95-4513-aae3-a30f6f923c50",
   "metadata": {},
   "source": [
    "依赖部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94874cd-cd9b-4ec2-a955-55299b2d75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # 显示 Python 可执行文件路径\n",
    "# python_path = sys.executable\n",
    "# print(f\"当前 Python 路径: {python_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9bf6ce-0252-4b95-940d-9d044492e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !C:\\Users\\陈衍鑫\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b02695e-9032-4c29-8482-90de426121a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import string\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14265c-ea51-4dbc-9d48-3ba001680122",
   "metadata": {},
   "source": [
    "读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a727210b-732b-4151-9db2-20c5da53456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 按数据集提示解析 JSON 数据\n",
    "def parse(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "# 将数据转换为 PD\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient=\"index\")\n",
    "\n",
    "\n",
    "# 读取和展示 meta_Sports_and_Outdoors.json 数据\n",
    "meta_file_path = \"Data/meta_Sports_and_Outdoors.json\"\n",
    "meta_df = getDF(meta_file_path)\n",
    "\n",
    "# 读取和展示 Sports_and_Outdoors_5.json 数据\n",
    "reviews_file_path = \"Data/Sports_and_Outdoors_5.json\"\n",
    "review_df = getDF(reviews_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37573093-3a1a-41f3-a22d-fad0695f518b",
   "metadata": {},
   "source": [
    "数据展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78937f49-1df1-4d64-9de5-1b012c50ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前5个meta数据:\n",
      "                                            category tech1  \\\n",
      "0  [Sports & Outdoors, Sports & Fitness, Other Sp...         \n",
      "1  [Sports & Outdoors, Sports & Fitness, Other Sp...         \n",
      "2  [Sports & Outdoors, Sports & Fitness, Other Sp...         \n",
      "3  [Sports & Outdoors, Sports & Fitness, Other Sp...         \n",
      "4  [Sports & Outdoors, Sports & Fitness, Other Sp...         \n",
      "\n",
      "                                         description fit  \\\n",
      "0  [3 layers of super-soft polyester tulle can be...       \n",
      "1  [3 layers of super-soft polyester tulle can be...       \n",
      "2  [3 layers of super-soft polyester tulle can be...       \n",
      "3                                             [TUtu]       \n",
      "4  [Dance tutu for girls ages 2-8 years. Perfect ...       \n",
      "\n",
      "                                     title  \\\n",
      "0   Adult Tutu Assorted Colors (Turquoise)   \n",
      "1  Bububibi Adult Ballet Tutu Cheetah Pink   \n",
      "2            Girls Ballet Tutu Neon Orange   \n",
      "3         Girls Ballet Tutu Zebra Hot Pink   \n",
      "4              Girls Ballet Tutu Neon Blue   \n",
      "\n",
      "                                            also_buy tech2     brand  \\\n",
      "0                                                 []        BubuBibi   \n",
      "1                                                 []        BubuBibi   \n",
      "2                                       [B01MYHD3LV]         Unknown   \n",
      "3                                                 []            Tutu   \n",
      "4  [B009THXHPA, B00BEU1ZBI, B079PYLZTM, B00BEU1ZR...        Bububibi   \n",
      "\n",
      "                                             feature  \\\n",
      "0  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "1  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "2  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "3                                             [Tutu]   \n",
      "4  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "\n",
      "                                                rank  \\\n",
      "0             712,899 in Clothing, Shoes & Jewelry (   \n",
      "1             712,899 in Clothing, Shoes & Jewelry (   \n",
      "2  [>#924,198 in Toys & Games (See Top 100 in Toy...   \n",
      "3                   1,314,199 in Sports & Outdoors (   \n",
      "4  [>#393,597 in Toys & Games (See Top 100 in Toy...   \n",
      "\n",
      "                                           also_view  \\\n",
      "0  [B071LQWQBQ, B00M14DG0O, B076GTTY9W, B07BSZ4GY...   \n",
      "1                                                 []   \n",
      "2  [B0152HBC52, B07C29WWV1, B01M0BIP9H, B00VXJHFR...   \n",
      "3                                       [0000013714]   \n",
      "4  [B009THXHPA, B079PYLZTM, B079PQPLT3, B00UP6A3R...   \n",
      "\n",
      "                                            main_cat similar_item  \\\n",
      "0  <img src=\"https://images-na.ssl-images-amazon....                \n",
      "1  <img src=\"https://images-na.ssl-images-amazon....                \n",
      "2                                       Toys & Games                \n",
      "3                                  Sports & Outdoors                \n",
      "4                                       Toys & Games                \n",
      "\n",
      "                                                date   price        asin  \\\n",
      "0  <div class=\"a-fixed-left-grid a-spacing-none\">...  $11.80  0000032042   \n",
      "1  <div class=\"a-fixed-left-grid a-spacing-none\">...  $11.97  0000032069   \n",
      "2                                                      $5.70  0000031860   \n",
      "3                                                      $7.50  0000031852   \n",
      "4                                                      $6.54  0000031895   \n",
      "\n",
      "  imageURL imageURLHighRes details  \n",
      "0       []              []     NaN  \n",
      "1       []              []     NaN  \n",
      "2       []              []     NaN  \n",
      "3       []              []     NaN  \n",
      "4       []              []     NaN  \n",
      "\n",
      "前5个review数据:\n",
      "   overall  verified   reviewTime      reviewerID        asin    reviewerName  \\\n",
      "0      5.0      True   06 3, 2015  A180LQZBUWVOLF  0000032034      Michelle A   \n",
      "1      1.0      True   04 1, 2015   ATMFGKU5SVEYY  0000032034       Crystal R   \n",
      "2      5.0      True  01 13, 2015  A1QE70QBJ8U6ZG  0000032034  darla Landreth   \n",
      "3      5.0      True  12 23, 2014  A22CP6Z73MZTYU  0000032034        L. Huynh   \n",
      "4      4.0      True  12 15, 2014  A22L28G8NRNLLN  0000032034         McKenna   \n",
      "\n",
      "                                          reviewText  \\\n",
      "0            What a spectacular tutu! Very slimming.   \n",
      "1  What the heck? Is this a tutu for nuns? I know...   \n",
      "2                  Exactly what we were looking for!   \n",
      "3  I used this skirt for a Halloween costume and ...   \n",
      "4  This is thick enough that you can't see throug...   \n",
      "\n",
      "                                             summary  unixReviewTime style  \\\n",
      "0                                         Five Stars      1433289600   NaN   \n",
      "1                          Is this a tutu for nuns?!      1427846400   NaN   \n",
      "2                                         Five Stars      1421107200   NaN   \n",
      "3  I liked that the elastic waist didn't dig in (...      1419292800   NaN   \n",
      "4  This is thick enough that you can't see throug...      1418601600   NaN   \n",
      "\n",
      "  vote image  \n",
      "0  NaN   NaN  \n",
      "1  NaN   NaN  \n",
      "2  NaN   NaN  \n",
      "3  NaN   NaN  \n",
      "4  NaN   NaN  \n",
      "meta_Sports_and_Outdoors.json 的属性名列表:\n",
      "['category', 'tech1', 'description', 'fit', 'title', 'also_buy', 'tech2', 'brand', 'feature', 'rank', 'also_view', 'main_cat', 'similar_item', 'date', 'price', 'asin', 'imageURL', 'imageURLHighRes', 'details']\n",
      "\n",
      "Sports_and_Outdoors_5.json 的属性名列表:\n",
      "['overall', 'verified', 'reviewTime', 'reviewerID', 'asin', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime', 'style', 'vote', 'image']\n"
     ]
    }
   ],
   "source": [
    "print(\"前5个meta数据:\")\n",
    "print(meta_df.head(5))\n",
    "\n",
    "print(\"\\n前5个review数据:\")\n",
    "print(review_df.head(5))\n",
    "\n",
    "# 提取属性名列表\n",
    "meta_columns = meta_df.columns.tolist()\n",
    "review_columns = review_df.columns.tolist()\n",
    "\n",
    "print(\"meta_Sports_and_Outdoors.json 的属性名列表:\")\n",
    "print(meta_columns)\n",
    "\n",
    "print(\"\\nSports_and_Outdoors_5.json 的属性名列表:\")\n",
    "print(review_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6c3f4-9945-4715-b0f4-a8f6c802e618",
   "metadata": {},
   "source": [
    "多模态数据选取与数据清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a4c2b5-976e-4bb0-bcf0-3f5810fd6557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选取后的 meta 数据集：\n",
      "         asin                                    title  \\\n",
      "0  0000032042   Adult Tutu Assorted Colors (Turquoise)   \n",
      "1  0000032069  Bububibi Adult Ballet Tutu Cheetah Pink   \n",
      "2  0000031860            Girls Ballet Tutu Neon Orange   \n",
      "3  0000031852         Girls Ballet Tutu Zebra Hot Pink   \n",
      "4  0000031895              Girls Ballet Tutu Neon Blue   \n",
      "\n",
      "                                         description  \\\n",
      "0  [3 layers of super-soft polyester tulle can be...   \n",
      "1  [3 layers of super-soft polyester tulle can be...   \n",
      "2  [3 layers of super-soft polyester tulle can be...   \n",
      "3                                             [TUtu]   \n",
      "4  [Dance tutu for girls ages 2-8 years. Perfect ...   \n",
      "\n",
      "                                             feature  \\\n",
      "0  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "1  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "2  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "3                                             [Tutu]   \n",
      "4  [3 Layers - 100% Polyester Tulle, Hand Wash La...   \n",
      "\n",
      "                                            category     brand   price  \\\n",
      "0  [Sports & Outdoors, Sports & Fitness, Other Sp...  BubuBibi  $11.80   \n",
      "1  [Sports & Outdoors, Sports & Fitness, Other Sp...  BubuBibi  $11.97   \n",
      "2  [Sports & Outdoors, Sports & Fitness, Other Sp...   Unknown   $5.70   \n",
      "3  [Sports & Outdoors, Sports & Fitness, Other Sp...      Tutu   $7.50   \n",
      "4  [Sports & Outdoors, Sports & Fitness, Other Sp...  Bububibi   $6.54   \n",
      "\n",
      "  imageURL                                           also_buy  \\\n",
      "0       []                                                 []   \n",
      "1       []                                                 []   \n",
      "2       []                                       [B01MYHD3LV]   \n",
      "3       []                                                 []   \n",
      "4       []  [B009THXHPA, B00BEU1ZBI, B079PYLZTM, B00BEU1ZR...   \n",
      "\n",
      "                                           also_view  \\\n",
      "0  [B071LQWQBQ, B00M14DG0O, B076GTTY9W, B07BSZ4GY...   \n",
      "1                                                 []   \n",
      "2  [B0152HBC52, B07C29WWV1, B01M0BIP9H, B00VXJHFR...   \n",
      "3                                       [0000013714]   \n",
      "4  [B009THXHPA, B079PYLZTM, B079PQPLT3, B00UP6A3R...   \n",
      "\n",
      "                                                rank  \n",
      "0             712,899 in Clothing, Shoes & Jewelry (  \n",
      "1             712,899 in Clothing, Shoes & Jewelry (  \n",
      "2  [>#924,198 in Toys & Games (See Top 100 in Toy...  \n",
      "3                   1,314,199 in Sports & Outdoors (  \n",
      "4  [>#393,597 in Toys & Games (See Top 100 in Toy...  \n",
      "\n",
      "选取后的 review 数据集：\n",
      "       reviewerID        asin  overall  \\\n",
      "0  A180LQZBUWVOLF  0000032034      5.0   \n",
      "1   ATMFGKU5SVEYY  0000032034      1.0   \n",
      "2  A1QE70QBJ8U6ZG  0000032034      5.0   \n",
      "3  A22CP6Z73MZTYU  0000032034      5.0   \n",
      "4  A22L28G8NRNLLN  0000032034      4.0   \n",
      "\n",
      "                                             summary   reviewTime image  \n",
      "0                                         Five Stars   06 3, 2015   NaN  \n",
      "1                          Is this a tutu for nuns?!   04 1, 2015   NaN  \n",
      "2                                         Five Stars  01 13, 2015   NaN  \n",
      "3  I liked that the elastic waist didn't dig in (...  12 23, 2014   NaN  \n",
      "4  This is thick enough that you can't see throug...  12 15, 2014   NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈衍鑫\\AppData\\Local\\Temp\\ipykernel_10632\\919398401.py:55: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  meta_null_counts = meta_df.applymap(is_empty).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_Sports_and_Outdoors.json 的空缺值统计:\n",
      " asin                0\n",
      "title              15\n",
      "description    150771\n",
      "feature        158207\n",
      "category        68321\n",
      "brand           96973\n",
      "price          556560\n",
      "imageURL       485817\n",
      "also_buy       696170\n",
      "also_view      605315\n",
      "rank            19604\n",
      "dtype: int64\n",
      "\n",
      "meta_Sports_and_Outdoors.json 的总记录数:\n",
      " 962300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈衍鑫\\AppData\\Local\\Temp\\ipykernel_10632\\919398401.py:62: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  review_null_counts = review_df.applymap(is_empty).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sports_and_Outdoors_5.json 的空缺值统计:\n",
      " reviewerID          0\n",
      "asin                0\n",
      "overall             0\n",
      "summary           617\n",
      "reviewTime          0\n",
      "image         2775405\n",
      "dtype: int64\n",
      "\n",
      "Sports_and_Outdoors_5.json 的总记录数:\n",
      " 2839940\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# is_empty 函数，确保正确处理各种类型的空值\n",
    "def is_empty(value):\n",
    "    if value is None:\n",
    "        return True\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return True\n",
    "    if isinstance(value, str) and value.strip() == \"\":\n",
    "        return True\n",
    "    if isinstance(value, list) and len(value) == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 选取 meta_Sports_and_Outdoors.json 数据集的指定特征\n",
    "meta_selected_features = [\n",
    "    'asin',            # 产品ID\n",
    "    'title',           # 产品名称\n",
    "    'description',     # 产品描述\n",
    "    'feature',         # 产品特征\n",
    "    'category',      # 产品类别\n",
    "    'brand',           # 品牌\n",
    "    'price',           # 价格\n",
    "    'imageURL',        # 产品图片URL\n",
    "    'also_buy',        # 购买该商品的用户还购买了\n",
    "    'also_view',       # 浏览该商品的用户还浏览了\n",
    "    'rank'             # 销售排名\n",
    "]\n",
    "\n",
    "# 从 meta_df 中选取指定的特征属性\n",
    "meta_df = meta_df[meta_selected_features]\n",
    "\n",
    "# 显示选取后的前5行数据\n",
    "print(\"选取后的 meta 数据集：\")\n",
    "print(meta_df.head())\n",
    "\n",
    "# 选取 Sports_and_Outdoors_5.json 数据集的指定特征\n",
    "review_selected_features = [\n",
    "    'reviewerID',   # 评论者ID\n",
    "    'asin',         # 产品ID\n",
    "    'overall',      # 评分\n",
    "    'summary',   # 评论内容\n",
    "    'reviewTime',   # 评论时间\n",
    "    'image'         # 用户上传的图片\n",
    "]\n",
    "\n",
    "# 从 review_df 中选取指定的特征属性\n",
    "review_df = review_df[review_selected_features]\n",
    "\n",
    "# 显示选取后的前5行数据\n",
    "print(\"\\n选取后的 review 数据集：\")\n",
    "print(review_df.head())\n",
    "\n",
    "\n",
    "# 统计 meta_Sports_and_Outdoors.json 的空缺值和总数\n",
    "meta_null_counts = meta_df.applymap(is_empty).sum()\n",
    "meta_total_counts = len(meta_df)\n",
    "\n",
    "print(\"meta_Sports_and_Outdoors.json 的空缺值统计:\\n\", meta_null_counts)\n",
    "print(\"\\nmeta_Sports_and_Outdoors.json 的总记录数:\\n\", meta_total_counts)\n",
    "\n",
    "# 统计 Sports_and_Outdoors_5.json 的空缺值和总数\n",
    "review_null_counts = review_df.applymap(is_empty).sum()\n",
    "review_total_counts = len(review_df)\n",
    "\n",
    "print(\"\\nSports_and_Outdoors_5.json 的空缺值统计:\\n\", review_null_counts)\n",
    "print(\"\\nSports_and_Outdoors_5.json 的总记录数:\\n\", review_total_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffa113-977d-4a84-a9d4-300ec6d5d81c",
   "metadata": {},
   "source": [
    "特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658fac67-053d-4938-85fb-831d2ff61db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺少 'rank' 的数据项数量：19604\n",
      "meta_df 总记录数：962300\n",
      "需要删除的比例：2.04%\n",
      "缺少 'summary' 的数据项数量：617\n",
      "review_df 总记录数：2839940\n",
      "需要删除的比例：0.02%\n",
      "删除后 meta_df 的记录数：942696\n",
      "删除后 review_df 的记录数：2839323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈衍鑫\\AppData\\Local\\Temp\\ipykernel_10632\\3269293636.py:38: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  meta_null_counts = meta_df.applymap(is_empty).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_df 的空缺值统计:\n",
      " asin                0\n",
      "title              15\n",
      "description    146940\n",
      "feature        151839\n",
      "category        66238\n",
      "brand           94200\n",
      "price          541676\n",
      "imageURL       475694\n",
      "also_buy       679676\n",
      "also_view      590800\n",
      "rank                0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈衍鑫\\AppData\\Local\\Temp\\ipykernel_10632\\3269293636.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  review_null_counts = review_df.applymap(is_empty).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_df 的空缺值统计:\n",
      " reviewerID          0\n",
      "asin                0\n",
      "overall             0\n",
      "summary             0\n",
      "reviewTime          0\n",
      "image         2774812\n",
      "dtype: int64\n",
      "合并后的数据集大小： 2828532\n"
     ]
    }
   ],
   "source": [
    "# 定义 is_empty 函数，确保正确处理各种类型的空值\n",
    "def is_empty(value):\n",
    "    if value is None:\n",
    "        return True\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return True\n",
    "    if isinstance(value, str) and value.strip() == \"\":\n",
    "        return True\n",
    "    if isinstance(value, list) and len(value) == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 查看缺少 'rank' 的数据项数量\n",
    "missing_rank_count = meta_df['rank'].apply(is_empty).sum()\n",
    "total_meta_entries = len(meta_df)\n",
    "print(f\"缺少 'rank' 的数据项数量：{missing_rank_count}\")\n",
    "print(f\"meta_df 总记录数：{total_meta_entries}\")\n",
    "print(f\"需要删除的比例：{(missing_rank_count / total_meta_entries) * 100:.2f}%\")\n",
    "\n",
    "# 删除 'rank' 列为空的数据项\n",
    "meta_df = meta_df[~meta_df['rank'].apply(is_empty)].reset_index(drop=True)\n",
    "\n",
    "# 查看缺少 'summary' 的数据项数量\n",
    "missing_summary_count = review_df['summary'].apply(is_empty).sum()\n",
    "total_review_entries = len(review_df)\n",
    "print(f\"缺少 'summary' 的数据项数量：{missing_summary_count}\")\n",
    "print(f\"review_df 总记录数：{total_review_entries}\")\n",
    "print(f\"需要删除的比例：{(missing_summary_count / total_review_entries) * 100:.2f}%\")\n",
    "\n",
    "# 删除 'summary' 列为空的数据项\n",
    "review_df = review_df[~review_df['summary'].apply(is_empty)].reset_index(drop=True)\n",
    "\n",
    "# 删除后的数据集情况\n",
    "print(f\"删除后 meta_df 的记录数：{len(meta_df)}\")\n",
    "print(f\"删除后 review_df 的记录数：{len(review_df)}\")\n",
    "\n",
    "# 重新统计缺失值\n",
    "meta_null_counts = meta_df.applymap(is_empty).sum()\n",
    "print(\"meta_df 的空缺值统计:\\n\", meta_null_counts)\n",
    "\n",
    "review_null_counts = review_df.applymap(is_empty).sum()\n",
    "print(\"\\nreview_df 的空缺值统计:\\n\", review_null_counts)\n",
    "\n",
    "# 合并数据集\n",
    "merged_df = pd.merge(review_df, meta_df, on='asin', how='inner')\n",
    "print(\"合并后的数据集大小：\", len(merged_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc1aac6-ac84-43f5-86a0-b5906a2c2e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后数据集的属性列表:\n",
      "['reviewerID', 'asin', 'overall', 'summary', 'reviewTime', 'image', 'title', 'description', 'feature', 'category', 'brand', 'price', 'imageURL', 'also_buy', 'also_view', 'rank']\n",
      "\n",
      "合并后数据集的第一行数据:\n",
      "reviewerID                                        A180LQZBUWVOLF\n",
      "asin                                                  0000032034\n",
      "overall                                                      5.0\n",
      "summary                                               Five Stars\n",
      "reviewTime                                            06 3, 2015\n",
      "image                                                        NaN\n",
      "title                                   Adult Ballet Tutu Yellow\n",
      "description    [3 layers of super-soft polyester tulle can be...\n",
      "feature        [3 Layers - 100% Polyester Tulle, Hand Wash La...\n",
      "category       [Sports & Outdoors, Sports & Fitness, Other Sp...\n",
      "brand                                                   BubuBibi\n",
      "price                                                     $12.50\n",
      "imageURL                                                      []\n",
      "also_buy        [B07BSZ4GYZ, B00P87ZO2E, B07CPV7P5B, B07CJWWMQW]\n",
      "also_view      [B07CPV7P5B, B01JS7F3WU, B074WJ2C43, B01GYA3L2...\n",
      "rank                      736,438 in Clothing, Shoes & Jewelry (\n",
      "Name: 0, dtype: object\n",
      "\n",
      "合并后数据集的缺失值统计:\n",
      "reviewerID           0\n",
      "asin                 0\n",
      "overall              0\n",
      "summary              0\n",
      "reviewTime           0\n",
      "image          2764261\n",
      "title                0\n",
      "description          0\n",
      "feature              0\n",
      "category             0\n",
      "brand                0\n",
      "price                0\n",
      "imageURL             0\n",
      "also_buy             0\n",
      "also_view            0\n",
      "rank                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 显示合并后数据集的属性列表\n",
    "print(\"合并后数据集的属性列表:\")\n",
    "print(merged_df.columns.tolist())\n",
    "\n",
    "# 显示合并后数据集的第一行数据\n",
    "print(\"\\n合并后数据集的第一行数据:\")\n",
    "print(merged_df.iloc[0])\n",
    "\n",
    "# 统计合并后数据集的缺失值\n",
    "merged_null_counts = merged_df.isnull().sum()\n",
    "print(\"\\n合并后数据集的缺失值统计:\")\n",
    "print(merged_null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55869683-0ab3-4d64-b77e-f36aaf4aefaa",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "388ef985-387e-43d2-a644-7c42bdbe43d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'price_num' 字段转换后缺失值数量：0\n",
      "已用中位数 15.36 填充 'price_num' 字段的缺失值。\n",
      "'rank' 字段的数据类型： object\n",
      "'rank_num' 字段缺失值数量：198429\n",
      "已用最大值 26324490.0 填充 'rank_num' 字段的缺失值。\n",
      "'reviewTime' 转换为日期类型后缺失值数量：0\n",
      "标准化后的 'price_norm' 和 'rank_norm'：\n",
      "         price_norm     rank_norm\n",
      "count  2.828532e+06  2.828532e+06\n",
      "mean   5.148706e-04  7.667761e-02\n",
      "std    2.364790e-03  2.540335e-01\n",
      "min    0.000000e+00  0.000000e+00\n",
      "25%    2.792338e-04  5.120328e-04\n",
      "50%    3.312394e-04  2.711886e-03\n",
      "75%    4.076295e-04  1.197322e-02\n",
      "max    1.000000e+00  1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# 处理 'price' 字段\n",
    "\n",
    "# 去除美元符号和逗号，创建新的数值型字段 'price_num'\n",
    "merged_df['price_num'] = merged_df['price'].replace('[\\$,]', '', regex=True)\n",
    "\n",
    "# 转换为浮点数\n",
    "merged_df['price_num'] = pd.to_numeric(merged_df['price_num'], errors='coerce')\n",
    "\n",
    "# 检查是否有转换失败的值\n",
    "price_missing = merged_df['price_num'].isnull().sum()\n",
    "print(f\"'price_num' 字段转换后缺失值数量：{price_missing}\")\n",
    "\n",
    "# 如果存在缺失值，用中位数填充\n",
    "median_price_num = merged_df['price_num'].median()\n",
    "merged_df['price_num'] = merged_df['price_num'].fillna(median_price_num)\n",
    "print(f\"已用中位数 {median_price_num} 填充 'price_num' 字段的缺失值。\")\n",
    "\n",
    "# 处理 'rank' 字段\n",
    "\n",
    "# 检查 'rank' 字段的数据类型\n",
    "print(\"'rank' 字段的数据类型：\", merged_df['rank'].dtype)\n",
    "\n",
    "# 定义一个函数来提取 'rank' 中的数字部分\n",
    "def extract_rank_number(rank_str):\n",
    "    if isinstance(rank_str, str):\n",
    "        # 使用正则表达式提取数字部分\n",
    "        match = re.search(r'([\\d,]+)', rank_str)\n",
    "        if match:\n",
    "            # 去除逗号，转换为整数\n",
    "            rank_num = int(match.group(1).replace(',', ''))\n",
    "            return rank_num\n",
    "    return None  # 如果无法提取数字，返回 None\n",
    "\n",
    "# 应用提取函数，创建新的数值型 'rank_num' 字段\n",
    "merged_df['rank_num'] = merged_df['rank'].apply(extract_rank_number)\n",
    "\n",
    "# 检查提取后的 'rank_num' 字段的缺失值数量\n",
    "rank_num_missing = merged_df['rank_num'].isnull().sum()\n",
    "print(f\"'rank_num' 字段缺失值数量：{rank_num_missing}\")\n",
    "\n",
    "if rank_num_missing > 0:\n",
    "    # 用排名的最大值（表示最低排名）来填充缺失值\n",
    "    max_rank_num = merged_df['rank_num'].max()\n",
    "    merged_df['rank_num'] = merged_df['rank_num'].fillna(max_rank_num)\n",
    "    print(f\"已用最大值 {max_rank_num} 填充 'rank_num' 字段的缺失值。\")\n",
    "else:\n",
    "    print(\"'rank_num' 字段无缺失值。\")\n",
    "\n",
    "# 转换 'reviewTime' 字段为日期类型\n",
    "merged_df['reviewTime'] = pd.to_datetime(merged_df['reviewTime'], errors='coerce')\n",
    "\n",
    "# 检查是否有转换失败的值\n",
    "reviewTime_missing = merged_df['reviewTime'].isnull().sum()\n",
    "print(f\"'reviewTime' 转换为日期类型后缺失值数量：{reviewTime_missing}\")\n",
    "\n",
    "# 如果存在缺失值，删除对应行\n",
    "merged_df = merged_df.dropna(subset=['reviewTime']).reset_index(drop=True)\n",
    "\n",
    "# 标准化 'price_num' 和 'rank_num' 字段，创建新的标准化字段 'price_norm' 和 'rank_norm'\n",
    "scaler = MinMaxScaler()\n",
    "merged_df[['price_norm', 'rank_norm']] = scaler.fit_transform(merged_df[['price_num', 'rank_num']])\n",
    "\n",
    "# 检查标准化后的值\n",
    "print(\"标准化后的 'price_norm' 和 'rank_norm'：\")\n",
    "print(merged_df[['price_norm', 'rank_norm']].describe())\n",
    "\n",
    "\n",
    "# 保留原始的 'price' 和 'rank' 字段，删除中间的数值型字段\n",
    "merged_df = merged_df.drop(columns=['price_num', 'rank_num'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d364504-aa12-4699-b2f6-f287e5c556c0",
   "metadata": {},
   "source": [
    "类别数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243bd384-782d-4be7-b7e7-6a97d14a062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 填充缺失值\n",
    "merged_df['brand'] = merged_df['brand'].fillna('unknown')\n",
    "\n",
    "# 进行标签编码\n",
    "le_brand = LabelEncoder()\n",
    "merged_df['brand_encoded'] = le_brand.fit_transform(merged_df['brand'])\n",
    "\n",
    "# 如果 'category' 是列表，取第一个类别\n",
    "merged_df['category_first'] = merged_df['category'].apply(lambda x: x[0][-1] if isinstance(x, list) and len(x) > 0 else 'unknown')\n",
    "\n",
    "# 进行标签编码\n",
    "le_category = LabelEncoder()\n",
    "merged_df['category_encoded'] = le_category.fit_transform(merged_df['category_first'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8c2fa-72c8-49cb-9ada-7cad219f57eb",
   "metadata": {},
   "source": [
    "文本数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb52e02-3874-468c-9950-928b971b7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, list):\n",
    "        # 如果输入是列表，将列表元素连接成一个字符串，元素之间用空格分隔\n",
    "        text = ' '.join(text)\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # 转为小写\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # 去除标点符号\n",
    "        text = re.sub(r'\\s+', ' ', text)  # 合并多个空格\n",
    "        text = text.strip()  # 去除首尾空格\n",
    "        return text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 清洗文本字段\n",
    "merged_df['title_clean'] = merged_df['title'].apply(clean_text)\n",
    "merged_df['description_clean'] = merged_df['description'].apply(clean_text)\n",
    "merged_df['summary_clean'] = merged_df['summary'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43185989-d334-4c66-b5c7-07fc50a3feaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题最大长度: 536\n",
      "描述最大长度: 50212\n",
      "摘要最大长度: 277\n",
      "标题平均长度: 65.40\n",
      "描述平均长度: 580.16\n",
      "摘要平均长度: 23.06\n",
      "合并后文本的最大长度: 50318\n",
      "合并后文本的平均长度: 670.62\n"
     ]
    }
   ],
   "source": [
    "# 查看每个字段的最大长度和平均长度\n",
    "max_title_length = merged_df['title_length'].max()\n",
    "max_description_length = merged_df['description_length'].max()\n",
    "max_summary_length = merged_df['summary_length'].max()\n",
    "\n",
    "mean_title_length = merged_df['title_length'].mean()\n",
    "mean_description_length = merged_df['description_length'].mean()\n",
    "mean_summary_length = merged_df['summary_length'].mean()\n",
    "\n",
    "print(f\"标题最大长度: {max_title_length}\")\n",
    "print(f\"描述最大长度: {max_description_length}\")\n",
    "print(f\"摘要最大长度: {max_summary_length}\")\n",
    "\n",
    "print(f\"标题平均长度: {mean_title_length:.2f}\")\n",
    "print(f\"描述平均长度: {mean_description_length:.2f}\")\n",
    "print(f\"摘要平均长度: {mean_summary_length:.2f}\")\n",
    "\n",
    "# 合并文本字段\n",
    "merged_df['combined_text'] = merged_df['title_clean'] + ' ' + merged_df['description_clean'] + ' ' + merged_df['summary_clean']\n",
    "\n",
    "# 计算合并后文本的长度\n",
    "merged_df['combined_length'] = merged_df['combined_text'].apply(len)\n",
    "\n",
    "# 查看合并后文本的最大长度和平均长度\n",
    "max_combined_length = merged_df['combined_length'].max()\n",
    "mean_combined_length = merged_df['combined_length'].mean()\n",
    "\n",
    "print(f\"合并后文本的最大长度: {max_combined_length}\")\n",
    "print(f\"合并后文本的平均长度: {mean_combined_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade0a7f-d6e2-4168-8810-02604bcc0eae",
   "metadata": {},
   "source": [
    "特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab031e-fb36-4598-80d4-6b057291fa1f",
   "metadata": {},
   "source": [
    "文本工程提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47f382a3-96a5-47af-89cc-5fa90f03dac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b6cdf5f4e649529296b9c5910e1a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈衍鑫\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\陈衍鑫\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f131b99a8648aaa74d5b0fb400ea77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d3a1d311cd4a76ae8b820073f209d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074b125496304e6899addd92da6d730a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ac821d70ff4aaeb5045ef3c8ca805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "文本嵌入处理中:   0%|                                                           | 448/353567 [00:34<7:29:04, 13.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m total_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_inputs \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader, total\u001b[38;5;241m=\u001b[39mtotal_batches, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m文本嵌入处理中\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     44\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_inputs)\n\u001b[0;32m     45\u001b[0m         batch_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [23], line 30\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[1;32m---> 30\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2868\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2867\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2868\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2956\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   2951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2952\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2953\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2954\u001b[0m         )\n\u001b[0;32m   2955\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2957\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2958\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2959\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2960\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2961\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2962\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2963\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2964\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2965\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   2966\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2967\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2968\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2969\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2970\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2971\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2972\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2973\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2974\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   2975\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2976\u001b[0m     )\n\u001b[0;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2979\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2980\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2998\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2999\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3158\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3149\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3150\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3151\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3156\u001b[0m )\n\u001b[1;32m-> 3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3159\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3160\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3161\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3162\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3163\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3164\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3165\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3166\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3167\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3168\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3169\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3170\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3171\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3172\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3173\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3174\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3175\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3176\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   3177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3178\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:888\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 888\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    889\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    890\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:855\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 855\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:662\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    660\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()  \u001b[38;5;66;03m# don't split on any of the added tokens\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;66;03m# \"This is something<special_token_1>  else\"\u001b[39;00m\n\u001b[1;32m--> 662\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_trie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:234\u001b[0m, in \u001b[0;36mTrie.split\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    230\u001b[0m         to_remove\u001b[38;5;241m.\u001b[39madd(start)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# Either clearing the full start (we found a real match)\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Or clearing only the partial matches that didn't work.\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[0;32m    235\u001b[0m     states \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器，并将模型移动到 GPU\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.to(device)  # 移动模型到 GPU\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "# 创建自定义数据集\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "# 定义批处理函数\n",
    "def collate_fn(batch):\n",
    "    # tokenizer 会自动处理批量文本的 padding 和编码\n",
    "    inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # 将输入张量移动到 GPU\n",
    "    return inputs\n",
    "\n",
    "# 创建 DataLoader\n",
    "texts = merged_df['combined_text'].tolist()\n",
    "dataset = TextDataset(texts)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)  \n",
    "\n",
    "# 进行批量嵌入计算\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for batch_inputs in data_loader:\n",
    "        outputs = model(**batch_inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # 取最后一层的平均池化结果\n",
    "        embeddings.append(batch_embeddings.cpu())\n",
    "\n",
    "# 将所有批量的嵌入拼接起来\n",
    "embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "\n",
    "# 将嵌入结果添加到 DataFrame 中\n",
    "merged_df['text_embedding'] = list(embeddings)\n",
    "\n",
    "print(merged_df[['combined_text', 'text_embedding']].head())\n",
    "merged_df.to_pickle('processed_merged_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb0c20-e4f6-4ea3-8877-849ffd817bc5",
   "metadata": {},
   "source": [
    "类别和品牌嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4f71c-8651-4353-bd4d-4501a1d0da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 获取类别和品牌的总数\n",
    "num_categories = merged_df['category_encoded'].nunique()\n",
    "num_brands = merged_df['brand_encoded'].nunique()\n",
    "\n",
    "# 定义嵌入层，设定嵌入维度为 32\n",
    "category_embedding_layer = nn.Embedding(num_categories, 32)\n",
    "brand_embedding_layer = nn.Embedding(num_brands, 32)\n",
    "\n",
    "# 将编码映射为嵌入向量\n",
    "merged_df['category_embedding'] = merged_df['category_encoded'].apply(lambda x: category_embedding_layer(torch.tensor(x)).detach().numpy())\n",
    "merged_df['brand_embedding'] = merged_df['brand_encoded'].apply(lambda x: brand_embedding_layer(torch.tensor(x)).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aaeec9-d2ea-47ad-aace-148eee7ac582",
   "metadata": {},
   "source": [
    "数值特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210c26a-3a6e-4521-82dc-ec40d6ad76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数值特征转换为 numpy 数组\n",
    "merged_df['price_feature'] = merged_df['price'].values\n",
    "merged_df['rank_feature'] = merged_df['rank'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26cd048-08e7-4af6-a468-df1a06d42f14",
   "metadata": {},
   "source": [
    "图像特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8d7bc-6d93-4d05-91ba-8dc9e3d95dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 加载预训练的 ResNet50 模型\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50.eval()\n",
    "\n",
    "# 定义图像预处理步骤\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # 可添加归一化步骤\n",
    "])\n",
    "\n",
    "# 定义图像特征提取函数\n",
    "def get_image_embedding(image_urls):\n",
    "    if not image_urls:  # 如果没有图片，返回零向量\n",
    "        return np.zeros(2048)\n",
    "    image_embeddings = []\n",
    "    for url in image_urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            img = image_transform(img).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                features = resnet50(img)\n",
    "            image_embeddings.append(features.squeeze().numpy())\n",
    "        except:\n",
    "            continue\n",
    "    if image_embeddings:\n",
    "        return np.mean(image_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(2048)\n",
    "\n",
    "# 提取图像特征\n",
    "merged_df['image_embedding'] = merged_df['imageURL'].apply(get_image_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f5b0e-e25c-4373-9bdf-392fa4e8c4ee",
   "metadata": {},
   "source": [
    "关联商品特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f7b0d-e669-4e8e-a8cc-8c2a82922a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算关联商品的数量\n",
    "merged_df['also_buy_count'] = merged_df['also_buy'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "merged_df['also_view_count'] = merged_df['also_view'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# 可选择进一步处理，如归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "merged_df[['also_buy_count_norm', 'also_view_count_norm']] = scaler.fit_transform(merged_df[['also_buy_count', 'also_view_count']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f342b-addb-4b68-9db9-666a3c4e51f2",
   "metadata": {},
   "source": [
    "特征融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f28e0-6d84-498a-ac9c-edcba617f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(row):\n",
    "    features = np.concatenate([\n",
    "        row['title_embedding'],\n",
    "        row['description_embedding'],\n",
    "        row['summary_embedding'],\n",
    "        row['category_embedding'],\n",
    "        row['brand_embedding'],\n",
    "        np.array([row['price_feature'], row['rank_feature']]),\n",
    "        row['image_embedding'],\n",
    "        np.array([row['also_buy_count_norm'], row['also_view_count_norm']])\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "# 生成最终的特征向量\n",
    "merged_df['combined_features'] = merged_df.apply(combine_features, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecf934-e9a5-4b43-8227-563a688f1b3e",
   "metadata": {},
   "source": [
    "PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e520e0-9751-40e8-bcbe-bdf5994546a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 对文本特征进行降维\n",
    "pca_text = PCA(n_components=256)\n",
    "text_features = np.stack(merged_df['title_embedding'] + merged_df['description_embedding'] + merged_df['summary_embedding'])\n",
    "text_features_reduced = pca_text.fit_transform(text_features)\n",
    "merged_df['text_features_reduced'] = list(text_features_reduced)\n",
    "\n",
    "# 对图像特征进行降维\n",
    "pca_image = PCA(n_components=256)\n",
    "image_features = np.stack(merged_df['image_embedding'])\n",
    "image_features_reduced = pca_image.fit_transform(image_features)\n",
    "merged_df['image_features_reduced'] = list(image_features_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6c0e6-18db-4583-9618-f300b30ac823",
   "metadata": {},
   "source": [
    "重新组合特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60099e37-524f-442b-87be-8ad21d9f10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reduced_features(row):\n",
    "    features = np.concatenate([\n",
    "        row['text_features_reduced'],\n",
    "        row['category_embedding'],\n",
    "        row['brand_embedding'],\n",
    "        np.array([row['price_feature'], row['rank_feature']]),\n",
    "        row['image_features_reduced'],\n",
    "        np.array([row['also_buy_count_norm'], row['also_view_count_norm']])\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "# 生成降维后的特征向量\n",
    "merged_df['combined_features_reduced'] = merged_df.apply(combine_reduced_features, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf2993-6501-4a9b-b015-3ea9a7f7ed06",
   "metadata": {},
   "source": [
    "保存处理后的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4eeaf4-9b1f-43ff-904b-9127a389bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取模型输入\n",
    "X = np.stack(merged_df['combined_features_reduced'])\n",
    "y = merged_df['overall'].values  # 或者根据任务定义的标签\n",
    "\n",
    "# 保存到文件\n",
    "np.save('features.npy', X)\n",
    "np.save('labels.npy', y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (System)",
   "language": "python",
   "name": "my_system_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
